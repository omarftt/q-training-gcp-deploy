{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde53ddc-16f4-4c7d-8cd9-803c1777144e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "075e46c8-4f05-4cdd-85b4-89590abb48a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from typing import List\n",
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import (component, Input, Model, Output, Dataset, \n",
    "                        Artifact, OutputPath, ClassificationMetrics, \n",
    "                        Metrics, InputPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33defa91-6cba-4941-ad9b-f2875da9ca1a",
   "metadata": {},
   "source": [
    "## Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f206572a-6516-45e6-a169-ca400502d7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'config.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "    \n",
    "PIPELINE_DISPLAY_NAME = config.get('PIPELINE_DISPLAY_NAME', 'my_pipeline')\n",
    "PIPELINE_DESCRIPTION = config.get('PIPELINE_DESCRIPTION', 'pipeline challenge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a69af-eef7-4fe6-8205-8409b25c0347",
   "metadata": {},
   "source": [
    "## Creating components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f2efc4-3ca3-4c4f-9ae8-ddaaa83211cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component()\n",
    "def error_raising(msg: str) -> None:\n",
    "    raise(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f99a0cb6-c3bc-410e-a3f9-ed774d87e3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['pandas', 'google-cloud-bigquery', 'db-dtypes'])\n",
    "def load_from_bq(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    location: str,\n",
    "    df_data: Output[Dataset]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load data from a specified BigQuery table and return a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        dataset_id (str): The dataset ID within BigQuery.\n",
    "        table_id (str): The table ID within the dataset.\n",
    "        location (str): The location of the BigQuery dataset. Default is \"EU\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data from the BigQuery table.\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    try:\n",
    "        # Initiate the BigQuery client to connect with the project.\n",
    "        bq_client = bigquery.Client(project=project_id, location=location)\n",
    "        \n",
    "        # Load data from the BigQuery table.\n",
    "        dataset_ref = bq_client.dataset(dataset_id, project=project_id)\n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "        table = bq_client.get_table(table_ref)\n",
    "        rows = bq_client.list_rows(table)\n",
    "\n",
    "        # Convert to a pandas DataFrame.\n",
    "        df_loaded = rows.to_dataframe()\n",
    "\n",
    "        if not df_loaded.empty:\n",
    "            df_loaded.to_csv(df_data.path, index=False)\n",
    "        else:\n",
    "            raise ValueError(\"Table content is empty.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during table load: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "877a002a-7fcf-463c-92a5-8762b716d689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['pandas', 'google-cloud-storage', 'db-dtypes'])\n",
    "def load_from_gcs(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    file_path: str,\n",
    "    df_data: Output[Dataset],\n",
    "    file_format: str = \"csv\",\n",
    ") -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load data from a specified GCS bucket and file, and return a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        file_path (str): The path to the file within the GCS bucket.\n",
    "        file_format (str): The format of the file (e.g., \"csv\", \"json\"). Default is \"csv\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data from the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "\n",
    "    try:\n",
    "        # Initialize the GCS client.\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "\n",
    "        # Get the bucket and blob.\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_path)\n",
    "        file_bytes = blob.download_as_bytes()\n",
    "\n",
    "        # Load the file into a pandas DataFrame based on the specified format.\n",
    "        if file_format == \"csv\":\n",
    "            df_loaded = pd.read_csv(BytesIO(file_bytes))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please use 'csv'.\")\n",
    "\n",
    "        if not df_loaded.empty:\n",
    "            df_loaded.to_csv(df_data.path, index=False)\n",
    "        else:\n",
    "            raise ValueError(\"Table content is empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file load: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23bfa20f-a9be-429d-b796-de887687dca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['pandas', 'scikit-learn', 'db-dtypes'])\n",
    "def preprocess_data(\n",
    "    df_data: Input[Dataset],\n",
    "    train_data: Output[Dataset],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the input DataFrame by handling null values in the specified columns.\n",
    "\n",
    "    Args:\n",
    "        df_data (pd.DataFrame): Input DataFrame with potential null values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        df_imputed = pd.read_csv(df_data.path)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        columns_to_impute = ['Age', 'Annual_Income', 'Credit_Score', 'Loan_Amount', 'Number_of_Open_Accounts']\n",
    "        imputer = SimpleImputer(fill_value=0)\n",
    "        df_imputed[columns_to_impute] = imputer.fit_transform(df_imputed[columns_to_impute])\n",
    "\n",
    "        if not df_imputed.empty:\n",
    "            df_imputed.to_csv(train_data.path, index=False)\n",
    "        else:\n",
    "            raise ValueError(\"Table content is empty.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during preprocessing: {e}\")\n",
    "        raise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1067d27-9aaa-457e-836a-e4892943d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['pandas', 'scikit-learn', 'google-cloud-aiplatform', 'db-dtypes'])\n",
    "def train_and_save_model(\n",
    "    project_id: str,\n",
    "    source_project: str,\n",
    "    bucket_name: str,\n",
    "    region: str, \n",
    "    model_display_name: str,\n",
    "    train_data: Input[Dataset]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression model and save it to the Vertex AI Model Registry and Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame for training.\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        region (str): Region for Vertex AI.\n",
    "        model_display_name (str): Display name for the model in Vertex AI.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df = pd.read_csv(train_data.path)\n",
    "        \n",
    "        # Splitting the data into features and target\n",
    "        X = df.drop('Loan_Approval', axis=1)\n",
    "        y = df['Loan_Approval']\n",
    "\n",
    "        # Splitting the data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "        # Training the Logistic Regression model\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(y_pred)\n",
    "\n",
    "        # Save the model to a local file\n",
    "        model_filename = 'model.joblib'\n",
    "        joblib.dump(model, model_filename)\n",
    "        \n",
    "        # Upload the model file to Google Cloud Storage\n",
    "        storage_client = storage.Client(project=source_project)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(model_filename)\n",
    "        blob.upload_from_filename(model_filename)\n",
    "\n",
    "        print(f\"Model {model_filename} uploaded to Google Cloud Storage.\")\n",
    "        \n",
    "        # Initialize the Vertex AI client\n",
    "        aiplatform.init(project=source_project, location=region)\n",
    "        \n",
    "        # Upload the model to Vertex AI Model Registry\n",
    "        aiplatform.Model.upload(\n",
    "            display_name=model_display_name,\n",
    "            artifact_uri=f'gs://{bucket_name}',  # GCS bucket URI for Vertex AI Model Registry\n",
    "            serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest'\n",
    "        )\n",
    "\n",
    "        print(f\"Model {model_display_name} successfully uploaded to Vertex AI Model Registry.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training, upload to Vertex AI, or upload to Cloud Storage: {e}\")\n",
    "        raise  # Re-raise the exception to stop the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c592668-1487-4830-a0a0-94b735547119",
   "metadata": {},
   "source": [
    "## Creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41e21b55-5e30-426a-975a-7e2c16ef97f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_DISPLAY_NAME, \n",
    "    description=PIPELINE_DESCRIPTION\n",
    ")\n",
    "\n",
    "def main_pipeline(\n",
    "    data_source: str,\n",
    "    source_project: str,\n",
    "    source_dataset: str,\n",
    "    source_table: str,\n",
    "    source_bucket: str,\n",
    "    datafile_name: str,\n",
    "    train_project_id: str,\n",
    "    model_display_name: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    \n",
    "    with dsl.If(data_source == 'bigquery'):\n",
    "        load_data_op = load_from_bq(\n",
    "                                    project_id=source_project,\n",
    "                                    dataset_id=source_dataset,\n",
    "                                    table_id=source_table,\n",
    "                                    location=gcp_region,\n",
    "                                    ).set_display_name(\"Load data from BQ\")\n",
    "        preprocess_data_op = preprocess_data(\n",
    "                                        df_data=load_data_op.outputs['df_data']\n",
    "                                        ).after(load_data_op).set_display_name(\"Preprocessing data\")\n",
    "    \n",
    "        train_save_op = train_and_save_model(\n",
    "                                        project_id=train_project_id,\n",
    "                                        source_project=source_project,\n",
    "                                        bucket_name=source_bucket,\n",
    "                                        region=gcp_region, \n",
    "                                        model_display_name=model_display_name,\n",
    "                                        train_data=preprocess_data_op.outputs['train_data'], \n",
    "                                        ).after(preprocess_data_op).set_display_name(\"Training and saving model\")\n",
    "    with dsl.Elif(data_source == 'storage'):\n",
    "        load_data_op = load_from_gcs(\n",
    "                                    project_id=source_project,\n",
    "                                    bucket_name=source_bucket,\n",
    "                                    file_path=datafile_name,\n",
    "                                    file_format='csv'\n",
    "                                    ).set_display_name(\"Load data from GCS\")\n",
    "        \n",
    "        preprocess_data_op = preprocess_data(\n",
    "                                        df_data=load_data_op.outputs['df_data']\n",
    "                                        ).after(load_data_op).set_display_name(\"Preprocessing data\")\n",
    "    \n",
    "        train_save_op = train_and_save_model(\n",
    "                                        project_id=train_project_id, \n",
    "                                        source_project=source_project,\n",
    "                                        bucket_name=source_bucket,\n",
    "                                        region=gcp_region, \n",
    "                                        model_display_name=model_display_name,\n",
    "                                        train_data=preprocess_data_op.outputs['train_data'], \n",
    "                                        ).after(preprocess_data_op).set_display_name(\"Training and saving model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7841b1-9886-4e50-9adf-978ee719917f",
   "metadata": {},
   "source": [
    "## Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba50727d-f953-4e95-8d3a-cdf8173ea42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path='_execution_/compiled_pipeline.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31341c7-b10d-492d-9a3d-7f272eecb430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m121"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
